<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0065)http://facweb.cs.depaul.edu/sjost/csc423/documents/f-test-reg.htm -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>F-test for Regression</title>
<link rel="stylesheet" type="text/css" href="./F-test for Regression_files/stylepage.css">
<style type="text/css">
tr { vertical-align: top; }
td { text-align: left; }
.r { text-align: right; }
}
</style>
</head>

<body>
<p style="text-align: right">
<a href="http://facweb.cs.depaul.edu/sjost/csc423/documents.htm">To Documents</a>
</p>


<h2>The F-test for Linear Regression</h2>

<h3>Purpose</h3>
<ul type="disc">
The <strong>F-test for linear regression</strong> tests whether any of the
independent variables in a multiple linear regression model 
are significant.
</ul>

<p>&nbsp;</p>

<h3>Definitions for Regression with Intercept</h3>
<ul type="disc">
<li>n is the number of observations, p is the number of regression parameters.
<br>&nbsp;</li>

<li><strong>Corrected Sum of Squares for Model:</strong> SSM = <span style="font-size:130%; font-weight:bold">Σ</span><sub size="-1">i=1</sub><sup size="-1">n</sup>
                (y<sub size="-1">i</sub>^ - <span class="ov">y</span>)<sup size="-1">2</sup>,<br>
&nbsp;&nbsp;&nbsp; also called sum of squares for regression.
<br>&nbsp;</li>

<li><strong>Sum of Squares for Error:</strong> SSE = <span style="font-size:130%; font-weight:bold">Σ</span><sub size="-1">i=1</sub><sup size="-1">n</sup>
                (y<sub size="-1">i</sub> - y<sub size="-1">i</sub>^)<sup size="-1">2</sup>,<br>
&nbsp;&nbsp;&nbsp; also called sum of squares for residuals.
<br>&nbsp;</li>

<li><strong>Corrected Sum of Squares Total:</strong> &nbsp; SST = <span style="font-size:130%; font-weight:bold">Σ</span><sub size="-1">i=1</sub><sup size="-1">n</sup>
                (y<sub size="-1">i</sub> - <span class="ov">y</span>)<sup size="-1">2</sup><br>
&nbsp;&nbsp;&nbsp; This is the sample variance of the y-variable multiplied by n - 1.<br>&nbsp;</li>

<li>For multiple regression models, we have this remarkable property: SSM + SSE = SST.
<br>&nbsp;</li>

<li><strong>Corrected Degrees of Freedom for Model:</strong> &nbsp; DFM = p - 1
<br>&nbsp;</li>

<li><strong>Degrees of Freedom for Error:</strong> &nbsp; DFE = n - p
<br>&nbsp;</li>

<li><strong>Corrected Degrees of Freedom Total:</strong> &nbsp; DFT = n - 1<br>
&nbsp;&nbsp;&nbsp; Subtract 1 from n for the corrected degrees of freedom.<br>  
&nbsp;&nbsp;&nbsp; Horizontal line regression is the null hypothesis model.
<br>&nbsp;</li>

<li>For multiple regression models with intercept, DFM + DFE = DFT.
<br>&nbsp;</li>

<li><strong>Mean of Squares for Model:</strong> &nbsp; MSM = SSM / DFM
<br>&nbsp;</li>

<li><strong>Mean of Squares for Error:</strong> &nbsp; MSE = SSE / DFE<br>
&nbsp;&nbsp;&nbsp; The sample variance of the residuals.
<br>&nbsp;</li>

<!--
<li>In a manner analogous to Property 10 of <a href="technical-details/rv-props.pdf" target="new">
Properties of Random Variables</a>, which states that s<sup size="-1">2</sup> is unbiased for 
&sigma;<sup size="-1">2</sup>, it can be shown that MSE is unbiased for 
&sigma;<sup size="-1">2</sup> for multiple regression models.
<br />&nbsp;</li> -->

<li><strong>Mean of Squares Total:</strong> &nbsp; MST = SST / DFT<br>
&nbsp;&nbsp;&nbsp; The sample variance of the y-variable.
<br>&nbsp;</li>

<li>In general, a researcher wants the variation due to the model (MSM) to be large with respect to the
variation due to the residuals (MSE).
<br>&nbsp;</li>

<li><strong>Note: </strong>the definitions in this section are not valid for regression through the
origin models.  They require the use of uncorrected sums of squares.</li>
</ul>

<p>&nbsp;</p>

<!--
<h3>Definitions for Regression through the Origin</h3>

<ul type="disc">
<li>Linear regression models with intercept use <strong>corrected sums of squares</strong> for sum of squares for model and sum of squares total.
They are measured with respect to the sample mean <span class="ov">y</span> as shown in the previous section.
<br />&nbsp;</li>

<li>Regression through the origin uses uncorrected sums of squares for the sum of squares for model and the sum of squares total.
They are measured with respect to the y-axis as shown in the following definitions.
<br />&nbsp;</li>


<li>n is the number of observations, p is the number of regression parameters.
<br />&nbsp;</li>

<li><strong>Uncorrected Sum of Squares for Model:</strong> SSM = <span style="font-size:130%; font-weight:bold">&Sigma;</span><sub size="-1">i=1</sub><sup size="-1">n</sup>
                (y<sub size="-1">i</sub>^)<sup size="-1">2</sup>,<br />
&nbsp;&nbsp;&nbsp; also called sum of squares for regression.
<br />&nbsp;</li>

<li><strong>Sum of Squares for Error:</strong> SSM = <span style="font-size:130%; font-weight:bold">&Sigma;</span><sub size="-1">i=1</sub><sup size="-1">n</sup>
                (y<sub size="-1">i</sub> - y<sub size="-1">i</sub>^)<sup size="-1">2</sup>,<br />
&nbsp;&nbsp;&nbsp; also called sum of squares for residuals.
<br />&nbsp;</li>

<li><strong>Uncorrected Sum of Squares Total:</strong> &nbsp; SST <span style="font-size:130%; font-weight:bold">&Sigma;</span><sub size="-1">i=1</sub><sup size="-1">n</sup>
                (y<sub size="-1">i</sub>)<sup size="-1">2</sup>
<br />&nbsp;</li>

<li>For multiple regression models, with or without intercept, SSM + SSE = SST.
<br />&nbsp;</li>

<li><strong>Uncorrected Degrees of Freedom for Model:</strong> &nbsp; DFM = p
<br />&nbsp;</li>

<li><strong>Degrees of Freedom for Error:</strong> &nbsp; DFE = n - p
<br />&nbsp;</li>

<li><strong>Uncorrected Degrees of Freedom Total:</strong> &nbsp; DFM = n
<br />&nbsp;</li>

<li>For multiple regression models with or without intercept, DFM + DFE = DFT.
<br />&nbsp;</li>

<li><strong>Mean of Squares for Model:</strong> &nbsp; MSM = SSM / DFM
<br />&nbsp;</li>

<li><strong>Mean of Squares for Error:</strong> &nbsp; MSE = SSE / DFE</li>


</ul>

<p>&nbsp;</p>
-->


<h3>The F-test</h3>
<ul type="disc">

<li>For a multiple regression model with intercept, we want to test the following null hypothesis and alternative hypothesis:<br>&nbsp;<br>
<ul>
H<sub size="-1">0</sub>: &nbsp; β<sub size="-1">1</sub> = β<sub size="-1">2</sub> = ... = β<sub size="-1">p-1</sub> = 0<br>&nbsp;<br>
H<sub size="-1">1</sub>:  &nbsp; β<sub size="-1">j</sub> ≠ 0, for at least one value of j
</ul>&nbsp;<br>
This test is known as the overall <strong>F-test for regression</strong>.
<br>&nbsp;</li>



<li>Here are the five steps of the <strong>overall F-test for regression</strong> <br>&nbsp;<br>
<ol type="1">
<li>State the null and alternative hypotheses:<br>&nbsp;<br>
<ul>
H<sub size="-1">0</sub>: &nbsp; β<sub size="-1">1</sub> = β<sub size="-1">2</sub> = ... = β<sub size="-1">p-1</sub> = 0
<br>&nbsp;<br>
H<sub size="-1">1</sub>:  &nbsp; β<sub size="-1">j</sub> ≠ 0, for at least one value of j
</ul>&nbsp;</li>

<li>Compute the test statistic assuming that the null hypothesis is true:<br>&nbsp;<br>
<ul>
F = MSM / MSE = (explained variance) / (unexplained variance)
</ul>&nbsp;</li>
<li>Find a (1 - α)100% confidence interval I for (DFM, DFE) degrees of freedom using an F-table or statistical software.
<br>&nbsp;</li>
<li>Accept the null hypothesis if F ∈ I; reject it if F ∉ I.
<br>&nbsp;</li>
<li>Use statistical software to determine the p-value.</li>
</ol><br></li>

<li><strong>Practice Problem:</strong> &nbsp;For a multiple regression model 
with 35 observations and 9 independent variables (10 parameters), SSE = 134 and&nbsp; SSM = 289, test the 
null hypothesis that all of the regression parameters are zero at the 0.05 level.<br>&nbsp;<br>
Solution: DFE = n - p = 35 - 10 = 25 and DFM = p - 1 = 10 - 1 = 9.  Here are the five steps of the test of hypothesis:<br>&nbsp;<br>
<ol type="1">
<li>State the null and alternative hypothesis:<br>&nbsp;<br>
<ul>
H<sub size="-1">0</sub>: &nbsp; β<sub size="-1">1</sub> = β<sub size="-1">2</sub> = , ... , = β<sub size="-1">p-1</sub> = 0
<br>&nbsp;<br>
H<sub size="-1">1</sub>: &nbsp; β<sub size="-1">j</sub> ≠ 0 for some j
</ul><br></li>
<li>Compute the test statistic:<br>&nbsp;<br>
<ul>
F = MSM/MSE = (SSM/DFM) / (SSE/DFE) = (289/9) / (134/25) =  32.111 / 5.360 = 5.991
</ul><br></li>
<li>Find a (1 - 0.05)×100% confidence interval for the test statistic.  Look in the F-table at the
0.05 entry for 9 df in the numerator and 25 df in the denominator.  This entry is 2.28, so the 95% confidence
interval is [0, 2.34].&nbsp; This confidence interval can also be found using 
the R function call qf(0.95, 9, 25).<br>&nbsp;</li>

<li>Decide whether to accept or reject the null hypothesis: 5.991 ∉ [0, 2.28], so reject
H<sub size="-1">0</sub>.
<br>&nbsp;</li>

<li>Determine the p-value.  To obtain the exact p-value, use statistical software.  However, we can find a rough
approximation to the p-value by examining the other entries in the F-table for (9, 25) degrees of freedom:<br>&nbsp;<br>
<table border="1" cellpadding="5">
<tbody><tr> <th>Level</th> <th>Confidence Interval</th> <th>F-value</th> </tr>
<tr> <td>0.100</td> <td class="c">[0, 0.900]</td> <td class="c">1.89</td> </tr>
<tr> <td>0.050</td> <td class="c">[0, 0.950]</td> <td class="c">2.28</td> </tr>
<tr> <td>0.025</td> <td class="c">[0, 0.975]</td> <td class="c">2.68</td> </tr>
<tr> <td>0.010</td> <td class="c">[0, 0.990]</td> <td class="c">2.22</td> </tr>
<tr> <td>0.001</td> <td class="c">[0, 0.999]</td> <td class="c">4.71</td> </tr>
</tbody></table><br>
The F-value is 5.991, so the p-value must be less than 0.005.
<br>&nbsp;</li>
</ol></li>

<li>Verify the value of the F-statistic for the <a href="http://facweb.cs.depaul.edu/sjost/csc423/examples.htm#hamster">Hamster Example</a>.</li>
</ul>

<!--
<p>&nbsp;</p>

<h3>Technical Details for the Overall F-Test</h3>

<ul type="disc">
<li>If t<sub size="-1">1</sub>, t<sub size="-1">2</sub>, ... , t<sub size="-1">m</sub>, are independent, N(0, &sigma;<sup size="-1">2</sup>) random variables,
then <span style="font-size:130%; font-weight:bold">&Sigma;</span><sub size="-1">i=1</sub><sup size="-1">m</sup> t<sub size="-1">i</sub><sup size="-1">2</sup>
is a χ<sup size="-1">2</sup> (chi-squared) random variable with m degrees of freedom.
<br />&nbsp;</li>


<li>It can be shown that if H<sub size="-1">0</sub> is true and the residuals are unbiased, homoscedastic, independent, and normal:<br />&nbsp;<br />
<ol type="1">
<li>SSE / &sigma;<sup size="-1">2</sup> has a χ<sup size="-1">2</sup> distribution with DFE degrees of freedom.
<br />&nbsp;</li>

<li>SSM / &sigma;<sup size="-1">2</sup> has a χ<sup size="-1">2</sup> distribution with DFM degrees of freedom.
<br />&nbsp;</li>

<li>SSE and SSM are independent random variables.
</li></ol><br /></li>

<li>If u is a χ<sup size="-1">2</sup> random variable with n degrees of freedom, 
v is a χ<sup size="-1">2</sup> random variable with m degrees of freedom, and u 
and v are independent, then if F = (u/n)/(v/m) has an <strong>F distribution with (n,m) degrees of freedom</strong>.&nbsp; 
See the F-tables in the <a href="stat-tables/stat-tables.pdf" target="new">Statistical Tables</a>.
<br />&nbsp;</li>

<li>By the previous information, if H<sub size="-1">0</sub> is true, F = [(SSM/&sigma;)/DFM]/[(SSE/&sigma;)/DFE] has an F distribution with (DFM, DFE) 
degrees of freedom.
<br />&nbsp;</li>
<li>But F = [(SSM/σ)/DFM]/[(SSE/σ)/DFE] = (SSM/DFM)/(SSE/DFE) = MSM/MSE, so F is 
independent of &sigma;.
</li>
</ul>
-->
<p>&nbsp;</p>

<h3><a name="adjr2">The R<sup>2</sup> and Adjusted R<sup>2</sup> Values</a></h3><a name="adjr2">
<ul type="disc">
<li>For simple linear regression, R<sup>2</sup> is the square of the sample correlation r<sub size="-1">xy</sub>.
<br>&nbsp;</li>

<li>For multiple linear regression with intercept (which includes simple linear regression), it is defined as r<sup>2</sup> = SSM / SST.
<br>&nbsp;</li>

<li>In either case, R<sup>2</sup> indicates the proportion of variation in 
the y-variable that is due to variation in the x-variables.
<br>&nbsp;</li>

<li>Many researchers prefer the <strong>adjusted R<sup>2</sup> value</strong> =
<strong><span class="ov">R</span><sup>2</sup></strong> instead, which is penalized for having a large number of parameters in the model:
<br>&nbsp;<br>
<ul>
<span class="ov">R</span><sup size="-1">2</sup> = 
	1 - (1 - R<sup size="-1">2</sup>)(n - 1) / (n - p)
</ul><br></li>

<li>Here derivation of <span class="ov">R</span><sup size="-1">2</sup>: &nbsp;
R<sup size="-1">2</sup> is defined as 1 - SSE/SST or 1 - R<sup size="-1">2</sup> 
= SSE/SST. To take into account the number of regression parameters p, define 
the adjusted R-squared value as<br>&nbsp;<br>
<ul>
1 - <span class="ov">R</span><sup size="-1">2</sup> = MSE/MST,
</ul><br>
where MSE = SSE/DFE = SSE/(n - p) and MST = SST/DFT = SST/(n - 1).  Thus,<br>&nbsp;<br>
<ul>
<table>
<tbody><tr><td class="r">1 - <span class="ov">R</span><sup size="-1">2</sup></td> <td>=</td> <td>[SSE/(n - p)] / [SST/(n - 1)]</td> </tr>
<tr><td>&nbsp;</td> <td>=</td> <td>(SSE/SST)(n - 1) / (n - p)</td> </tr>
</tbody></table></ul>
so<br>
<ul>
<table>
<tbody><tr><td class="r"><span class="ov">R</span><sup size="-1">2</sup></td> 
	<td style="width: 13px">=</td> <td style="width: 242px">1 - (SSE/SST)(n - 1) 
	/ (n - p)</td> </tr>
<tr><td>&nbsp;</td> <td>=</td> <td style="width: 242px">1 - (1 - R<sup size="-1">2</sup>)(n - 1) / (n - p)</td> </tr>
</tbody></table><br></ul>
</li>

<li><strong>Practice Problem:</strong> A regression model has 9 independent variables, 47 observations, and R<sup>2</sup> = 0.879.<br>&nbsp;<br>
Ans: p = 10 and n = 47. <span class="ov">R</span><sup>2</sup> =  1 - (1 - R<sup size="-1">2</sup>)(n - 1) / (n - p) =
1 - (1 - 0.879)(47 - 1) / (47 - 10) =&nbsp; 0.8496.</li>
</ul>

<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>



</a></body></html>